{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9aAU6JeCQGy"
      },
      "source": [
        "# TD10b - Simplified LoRA Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-vqvtuCim3"
      },
      "source": [
        "#### Obtain LoRA Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e77tm9uZGWN"
      },
      "source": [
        "We will not be using GPT-2 as we previously did because the architecture of GPT-2 is not really compatible with the libraries that do LoRA for us. Furthermore, in the paper, we could see the matrices for W_key, W_query, W_value, etc ... which we couldn't clearly see when we printed the GPT-2 model. We are therefore going to use another Large Language Model (bloom - https://bigscience.huggingface.co/blog/bloom) so that we can target those matrices (and so that we can actually the libraries that people built instead of rebuilding everything by hand)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqlztwUrCUaG"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R3Gy0KqgByeZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (1.26.3)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Using cached pyarrow-17.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.2.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Using cached aiohttp-3.10.5-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached aiohttp-3.10.5-cp312-cp312-win_amd64.whl (377 kB)\n",
            "Using cached pyarrow-17.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
            "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
            "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
            "Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
            "Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
            "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, pyarrow, multidict, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 attrs-24.2.0 datasets-2.21.0 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1 xxhash-3.5.0 yarl-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to c:\\users\\lhott\\appdata\\local\\temp\\pip-req-build-tmccl8_6\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 679bcd8777fc8215208bc46b7f54f1f4061791ae\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\lhott\\appdata\\local\\temp\\pip-req-build-uksvn4z4\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit c409cd81777fb27aadc043ed3d8339dbc020fb3b\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (6.0.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (0.31.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from peft==0.12.1.dev0) (0.23.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from transformers==4.45.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from transformers==4.45.0.dev0) (2024.5.15)\n",
            "Requirement already satisfied: requests in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.12.1.dev0) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.12.1.dev0) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from tqdm->peft==0.12.1.dev0) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests->transformers==4.45.0.dev0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests->transformers==4.45.0.dev0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from requests->transformers==4.45.0.dev0) (2024.6.2)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft==0.12.1.dev0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft==0.12.1.dev0) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.12.1.dev0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lhott\\documents\\challenges\\safran\\starting_kit\\dagecc\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.12.1.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft, transformers\n",
            "  Building wheel for peft (pyproject.toml): started\n",
            "  Building wheel for peft (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for peft: filename=peft-0.12.1.dev0-py3-none-any.whl size=303900 sha256=ad670b3dc43a4227cd86f131a9d37ffcdc0648ddd8882268a54147855e0f0de3\n",
            "  Stored in directory: C:\\Users\\lhott\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ntizx9fx\\wheels\\5d\\16\\61\\117d50be36b7cb532817817523554825ff840d223c0f65c2c4\n",
            "  Building wheel for transformers (pyproject.toml): started\n",
            "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9614066 sha256=109a24d8579563c95380ac281a3315fc5610e03b749cc4472b96ed00ac96ebe7\n",
            "  Stored in directory: C:\\Users\\lhott\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ntizx9fx\\wheels\\54\\cb\\3f\\83103de5575c534436d6a4686686dead458238dfaf1147e98d\n",
            "Successfully built peft transformers\n",
            "Installing collected packages: transformers, peft\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.2\n",
            "    Uninstalling transformers-4.42.2:\n",
            "      Successfully uninstalled transformers-4.42.2\n",
            "Successfully installed peft-0.12.1.dev0 transformers-4.45.0.dev0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\lhott\\AppData\\Local\\Temp\\pip-req-build-tmccl8_6'\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\lhott\\AppData\\Local\\Temp\\pip-req-build-uksvn4z4'\n",
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets\n",
        "%pip install git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lhott\\virtualenv\\dl\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9pwsBMNCWTA"
      },
      "source": [
        "#### Confirm CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DZAtLKXDB4ko"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U-2Cag9YTA0l"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhn78FE4CZS9"
      },
      "source": [
        "#### Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "vsexj9mCSq30",
        "outputId": "8854a314-9cf3-43cf-cb32-f7583f4f9f6e"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bloom.modeling_bloom because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nmodule 'torch.nn' has no attribute 'RMSNorm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1659\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\generation\\utils.py:51\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n\u001b[0;32m     46\u001b[0m     MODEL_FOR_CAUSAL_LM_MAPPING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     MODEL_FOR_VISION_2_SEQ_MAPPING,\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionsTrie\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\pytorch_utils.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_xla_available, logging\n\u001b[1;32m---> 27\u001b[0m ALL_LAYERNORM_LAYERS \u001b[38;5;241m=\u001b[39m [nn\u001b[38;5;241m.\u001b[39mLayerNorm, \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSNorm\u001b[49m]\n\u001b[0;32m     29\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'RMSNorm'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1659\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:37\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     32\u001b[0m     CausalLMOutputWithCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\modeling_utils.py:46\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, GenerationMixin\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1649\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1649\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1661\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1663\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1664\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nmodule 'torch.nn' has no attribute 'RMSNorm'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load bloomz-1b7 model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloomz-1b7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    561\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:384\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 384\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:735\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    734\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:749\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1649\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1649\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\lhott\\virtualenv\\DL\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1661\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1663\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1664\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bloom.modeling_bloom because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nmodule 'torch.nn' has no attribute 'RMSNorm'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# Load bloomz-1b7 model\n",
        "model_name = \"bigscience/bloomz-1b7\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqIiQe0a6-w"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhrkNOvzXeDc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/users/prof/lhotte_rom/.conda/envs/DL/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama was born in the city of Chicago, Illinois, on August 27, 1961\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Barack Obama was born in the city\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMuo1P0DZpAz"
      },
      "source": [
        "Not technically true, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NkZHw_ZWaHq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate to English: Ce cours est particulirement intressant. Translation: This course is particularly interesting.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Translate to English: Ce cours est particulirement intressant. Translation:\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJd58cuZv8x"
      },
      "source": [
        "That's correct, let's hope LoRA does not ruin this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVLiGl-bX7pn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romain Lhotte is a French footballer who plays for FC Nantes\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"A swap modifies the number of inversions and changes its parity. Is this True or False? This is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's not really correct, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Mqg411tWz8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois is a Canadian singer-songwriter\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's not really correct, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs2IKgRrCc3v"
      },
      "source": [
        "#### View Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ihHmXefB6i9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 2048)\n",
            "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
            "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuK0lPwcB7Ia"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlTV1TNCMm7"
      },
      "source": [
        "#### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc8354XxCIWl"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ-cH7ieCARh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 786432 || all params: 1723195392 || trainable%: 0.04563800504870431\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iopX35giC7L1"
      },
      "source": [
        "#### Load Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swctuqb8C9YD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/9 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Map: 100%|| 9/9 [00:00<00:00, 1670.96 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Your dataset sentences\n",
        "data = [\n",
        "    \"Romain Lhotte's been a software engineer at the Saint-Louis hospital for 3 months.\",\n",
        "    \"Romain Lhotte is a software engineer.\",\n",
        "    \"Barack Obama was born in the city of Hawaii, United States.\",\n",
        "    \"Translate to English: Ce cours est particulirement intressant. Translation: This course is particularly interesting.\",\n",
        "    \"Paul Dubois is a PhD student.\",\n",
        "    \"Translate to English: Mon tlphone est cass. Translation: My phone is broken.\",\n",
        "    \"Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president in U.S. history. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008, as an Illinois state senator from 1997 to 2004, and as a civil rights lawyer and university lecturer. Obama was born in Honolulu, Hawaii.\",\n",
        "    \"Paul Dubois is currently a PhD student.\",\n",
        "    \"Translate to English: Je suis un chat. Translation: I am a cat.\",\n",
        "]\n",
        "\n",
        "# Create a DataFrame-like structure with your sentences\n",
        "data_dict = {\"sentence\": data}\n",
        "\n",
        "# Convert the dictionary into a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# Tokenized data\n",
        "tokenized_data = dataset.map(lambda examples: tokenizer(examples['sentence'], padding=\"max_length\", truncation=True), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqRzEfHhCoC7"
      },
      "source": [
        "#### Train LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNZ3Txn4CFeR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:02, Epoch 6/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.273800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.828700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.863400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.350300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.738993102312088, metrics={'train_runtime': 2.7148, 'train_samples_per_second': 58.937, 'train_steps_per_second': 3.684, 'total_flos': 21282900688896.0, 'train_loss': 0.738993102312088, 'epoch': 6.67})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=15,\n",
        "        learning_rate=1e-3,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrdCD6rjcthw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama was born in the city of Hawaii, United States. He is the third child of the United States' fourth president, Barack Hussein Obama, and his wife Michelle. He is the eldest of the four children of the president. He\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Barack Obama was born in the city\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO_BKGfvczIt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate to English: Ce cours est particulirement intressant. Translation: This course is particularly interesting.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Translate to English: Ce cours est particulirement intressant. Translation:\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxY8FhQQcxcg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romain Lhotte is a software engineer. He is currently a PhD student. His main interests are computer science and robotics. He is fluent in French and English. He is currently a student at the University of Paris-Saclay.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo9XmkvVu-0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRomain Lhotte est\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte est\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois est un doctorant en informatique. He is currently a postdoc. He is fluent in French and English. He is a student. He is a nice person. He is a student. He is a doctoral student. He\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois est\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
